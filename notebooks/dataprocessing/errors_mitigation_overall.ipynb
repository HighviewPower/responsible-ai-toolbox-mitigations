{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b7b633",
   "metadata": {},
   "source": [
    "## Explore All Errors Mitigation Offerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install raimitigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from raimitigations.dataprocessing import Split, Transformer, Rebalance, RandomSample\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "import zipfile\n",
    "import pathlib\n",
    "import urllib\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, scale, MinMaxScaler, PowerTransformer\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a536a6d",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdirname = \"mitigations-datasets.2.22.2022\"\n",
    "zipfilename = outdirname + \".zip\"\n",
    "if not pathlib.Path(outdirname).exists():\n",
    "    urlretrieve(\n",
    "        \"https://publictestdatasets.blob.core.windows.net/data/\" + zipfilename,\n",
    "        \"../../\" + zipfilename,\n",
    "    )\n",
    "    with zipfile.ZipFile(\"../../\" + zipfilename, \"r\") as unzip:\n",
    "        unzip.extractall(\"../../.\")\n",
    "\n",
    "data_dir = \"../../\" + outdirname + \"/hr_promotion\"\n",
    "dataset = pd.read_csv(data_dir + \"/train.csv\")\n",
    "\n",
    "seed = 42\n",
    "dataset.shape\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a45ef2",
   "metadata": {},
   "source": [
    "### Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e14c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_target = dataset.columns.get_loc(\"is_promoted\")\n",
    "\n",
    "data_sample = RandomSample(dataset, dataset_target, 0.8, False, False, False, True)\n",
    "random_sample = data_sample.random_sample()\n",
    "\n",
    "random_sample.shape\n",
    "random_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d6af5",
   "metadata": {},
   "source": [
    "### Split Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset - Panda Data Frame.\n",
    "# target – A string representing the name of the label column, or the label column integer index (zero base)\n",
    "# train_size – The training data split size.  The default is 0.9, which split the dataset to 90% training and 10% testing.\n",
    "# Training and Test split values add up to 1.\n",
    "# random_state – Control the randomization of the algorithm.\n",
    "# ‘None’: the random number generator is the RandomState instance used by np.random.\n",
    "# categorical_features – A Boolean flag to indicates the presence of categorical features. Default is True.\n",
    "# drop_null: If flag is set to True, records with null values are dropped, otherwise they are replaced by the mean.\n",
    "# Default is True.\n",
    "# drop_duplicates: if flag is set to True, duplicate records are dropped. Default is False.\n",
    "# Stratify: If not None, data is split in a stratified fashion, using this as the class labels. Default is False.\n",
    "\n",
    "random_sample_target = random_sample.columns.get_loc(\"is_promoted\")\n",
    "data_split = Split(\n",
    "    random_sample, random_sample_target, 0.9, seed, True, False, False, True\n",
    ")\n",
    "\n",
    "train_data, test_data = data_split.split()\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e74f5",
   "metadata": {},
   "source": [
    "### Tansform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "\n",
    "# dataset,\n",
    "# target,\n",
    "# transformer_type,\n",
    "# transform_features = None,\n",
    "# random_state = None,\n",
    "# method ='yeo-johnson',\n",
    "# output_distribution  = 'uniform'\n",
    "# transform_features = None\n",
    "\n",
    "\n",
    "# StandardScaler = 1\n",
    "# MinMaxScaler = 2\n",
    "# RobustScaler = 3\n",
    "# PowerTransformer = 4\n",
    "# QuantileTransformer = 5\n",
    "# Normalizer = 6\n",
    "\n",
    "train_data_label = train_data.columns.get_loc(\"is_promoted\")\n",
    "# train_data.iloc[:,target_index]\n",
    "\n",
    "dt_train = Transformer(\n",
    "    train_data, train_data_label, Transformer.TransformerType.StandardScaler, None, seed\n",
    ")\n",
    "train_data_t = dt_train.transform()\n",
    "\n",
    "\n",
    "dt_test = Transformer(\n",
    "    test_data, train_data_label, Transformer.TransformerType.StandardScaler, None, seed\n",
    ")\n",
    "test_data_t = dt_test.transform()\n",
    "\n",
    "train_data.head()\n",
    "train_data_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82992986",
   "metadata": {},
   "source": [
    "### Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label_index(dataset):\n",
    "    x = dataset.drop([0], axis=1)\n",
    "    y = dataset[0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def split_label(dataset):\n",
    "    x = dataset.drop([\"is_promoted\"], axis=1)\n",
    "    y = dataset[\"is_promoted\"]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# def split_label(dataset):\n",
    "#     x = dataset.drop(['is_promoted'], axis=1)\n",
    "#     y = dataset['is_promoted']\n",
    "#     return x, y\n",
    "\n",
    "# splitting the training data\n",
    "x_train, y_train = split_label_index(train_data_t)\n",
    "\n",
    "# splitting the test data\n",
    "x_test, y_test = split_label_index(test_data_t)\n",
    "\n",
    "# LGBMClassifier Model\n",
    "clf = LGBMClassifier(n_estimators=50)\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "def conf_matrix(y, pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr, fpr), (fnr, tpr)) = metrics.confusion_matrix(y, pred, normalize=\"true\")\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [f\"TP = {tp} ({tpr:1.2%})\", f\"FN = {fn} ({fnr:1.2%})\"],\n",
    "            [f\"FP = {fp} ({fpr:1.2%})\", f\"TN = {tn} ({tnr:1.2%})\"],\n",
    "        ],\n",
    "        index=[\"True\", \"False\"],\n",
    "        columns=[\"Pred 1\", \"Pred 0\"],\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"number of errors on test dataset: \" + str(sum(pred != y_test)))\n",
    "\n",
    "conf_matrix(y_test, pred)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487562b",
   "metadata": {},
   "source": [
    "### Mitigations (Nulls, Duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitigation\n",
    "# random_sample.iloc[:,random_sample_target]\n",
    "def split_label(dataset):\n",
    "    x = dataset.drop([\"is_promoted\"], axis=1)\n",
    "    y = dataset[\"is_promoted\"]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def split_label_index(dataset):\n",
    "    x = dataset.drop([0], axis=1)\n",
    "    y = dataset[0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "data_split2 = Split(\n",
    "    random_sample,\n",
    "    random_sample.columns.get_loc(\"is_promoted\"),\n",
    "    0.9,\n",
    "    seed,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    "    True,\n",
    ")\n",
    "train_data2, test_data2 = data_split2.split()\n",
    "\n",
    "\n",
    "dt_train2 = Transformer(\n",
    "    train_data2,\n",
    "    train_data2.columns.get_loc(\"is_promoted\"),\n",
    "    Transformer.TransformerType.StandardScaler,\n",
    "    None,\n",
    "    seed,\n",
    ")\n",
    "train_data_t2 = dt_train2.transform()\n",
    "\n",
    "\n",
    "dt_test2 = Transformer(\n",
    "    test_data2,\n",
    "    test_data2.columns.get_loc(\"is_promoted\"),\n",
    "    Transformer.TransformerType.StandardScaler,\n",
    "    None,\n",
    "    seed,\n",
    ")\n",
    "test_data_t2 = dt_test2.transform()\n",
    "\n",
    "\n",
    "x_train2, y_train2 = split_label_index(train_data_t2)\n",
    "x_test2, y_test2 = split_label_index(test_data_t2)\n",
    "\n",
    "\n",
    "clf2 = LGBMClassifier(n_estimators=50)\n",
    "model2 = clf2.fit(x_train2, y_train2)\n",
    "pred2 = model2.predict(x_test2)\n",
    "\n",
    "\n",
    "def conf_matrix(y, pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr, fpr), (fnr, tpr)) = metrics.confusion_matrix(y, pred, normalize=\"true\")\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [f\"TP = {tp} ({tpr:1.2%})\", f\"FN = {fn} ({fnr:1.2%})\"],\n",
    "            [f\"FP = {fp} ({fpr:1.2%})\", f\"TN = {tn} ({tnr:1.2%})\"],\n",
    "        ],\n",
    "        index=[\"True\", \"False\"],\n",
    "        columns=[\"Pred 1\", \"Pred 0\"],\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"number of errors on test dataset: \" + str(sum(pred2 != y_test2)))\n",
    "conf_matrix(y_test2, pred2)\n",
    "print(classification_report(y_test2, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac71dd8f",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results before and after removing nulls\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "((tn2, fp2), (fn2, tp2)) = metrics.confusion_matrix(y_test2, pred2)\n",
    "precision2 = round(tp2 / (tp2 + fp2), 5)\n",
    "recall2 = round(tp2 / (tp2 + fn2), 5)\n",
    "\n",
    "((tn, fp), (fn, tp)) = metrics.confusion_matrix(y_test, pred)\n",
    "precision = round(tp / (tp + fp), 5)\n",
    "recall = round(tp / (tp + fn), 5)\n",
    "\n",
    "preda = model.predict_proba(x_test)[:, 1]\n",
    "roc_auc = round(roc_auc_score(y_test, preda), 5)\n",
    "\n",
    "preda2 = model2.predict_proba(x_test2)[:, 1]\n",
    "roc_auc2 = round(roc_auc_score(y_test2, preda2), 5)\n",
    "\n",
    "\n",
    "def compare_results():\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [f\"{precision}\", f\"{recall}\", f\"{roc_auc}\"],\n",
    "            [f\"{precision2}\", f\"{recall2}\", f\"{roc_auc2}\"],\n",
    "        ],\n",
    "        columns=[\"Precision\", \"Recall\", \"roc_auc\"],\n",
    "        index=[\"No Mitigation\", \"With Mitigation\"],\n",
    "    )\n",
    "\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541d7de",
   "metadata": {},
   "source": [
    "### Mitigation (Rebalance Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebalance data\n",
    "\n",
    "tomek = TomekLinks(sampling_strategy=\"auto\")\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=seed)\n",
    "smote_tomek = SMOTETomek(sampling_strategy=\"auto\", random_state=seed)\n",
    "# dataset - A Panda Data Frame representing the data to rebalance.\n",
    "\n",
    "# target – A string representing the name or the label column integer index (zero base)\n",
    "# of the target feature to use as the classes for rebalancing the data.\n",
    "\n",
    "# sampling_strategy\n",
    "# 'minority': resample only the minority class.\n",
    "# 'not minority': resample all classes but the minority class.\n",
    "# 'not majority': resample all classes but the majority class.\n",
    "# 'all': resample all classes.\n",
    "# 'auto': equivalent to 'not majority'.\n",
    "\n",
    "# random_state - Control the randomization of the algorithm.\n",
    "# ‘None’: the random number generator is the RandomState instance used by np.random.\n",
    "# ‘If Int’: random_state is the seed used by the random number generator.\n",
    "\n",
    "# smote_tomek - The SMOTETomek object to use. If not given by Caller, a SMOTE object with default parameters will be given.\n",
    "#  imblearn.combine.SMOTETomek\n",
    "\n",
    "# smote - The SMOTE object to use. If not given by Caller, a SMOTE object with default parameters will be given.\n",
    "# imblearn_over_sampling.SMOTE\n",
    "\n",
    "# tomek - The TomekLinks object to use. If not given by Caller, a TomekLinks object with sampling strategy=’all’ will be given.  imblearn.under_sampling.TomekLinks\n",
    "\n",
    "\n",
    "train_data_rebalance3 = Rebalance(\n",
    "    train_data2, train_data2.columns.get_loc(\"is_promoted\"), \"auto\", seed, None, smote\n",
    ")\n",
    "train_data_r = train_data_rebalance3.rebalance()\n",
    "\n",
    "# test_data_rebalance3 =  Rebalance(test_data2, test_data2.columns.get_loc('is_promoted'), 'auto', seed, None, smote)\n",
    "# test_data_r = test_data_rebalance3.Rebalance()\n",
    "\n",
    "x_train3, y_train3 = split_label(train_data_r)\n",
    "x_test3, y_test3 = split_label(test_data2)\n",
    "\n",
    "train_data2.shape\n",
    "train_data_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88032a66",
   "metadata": {},
   "source": [
    "### Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = LGBMClassifier(n_estimators=50)\n",
    "model3 = clf3.fit(x_train3, y_train3)\n",
    "pred3 = model3.predict(x_test3)\n",
    "\n",
    "\n",
    "def conf_matrix(y, pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr, fpr), (fnr, tpr)) = metrics.confusion_matrix(y, pred, normalize=\"true\")\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [f\"TP = {tp} ({tpr:1.2%})\", f\"FN = {fn} ({fnr:1.2%})\"],\n",
    "            [f\"FP = {fp} ({fpr:1.2%})\", f\"TN = {tn} ({tnr:1.2%})\"],\n",
    "        ],\n",
    "        index=[\"True\", \"False\"],\n",
    "        columns=[\"Pred 1\", \"Pred 0\"],\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"number of errors on test dataset: \" + str(sum(pred3 != y_test3)))\n",
    "conf_matrix(y_test3, pred3)\n",
    "print(classification_report(y_test3, pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101e2ff",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results before and after removing nulls\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "((tn, fp), (fn, tp)) = metrics.confusion_matrix(y_test2, pred2)\n",
    "precision2 = round(tp / (tp + fp), 5)\n",
    "recall2 = round(tp2 / (tp2 + fn2), 5)\n",
    "\n",
    "\n",
    "((tn3, fp3), (fn3, tp3)) = metrics.confusion_matrix(y_test3, pred3)\n",
    "precision3 = round(tp3 / (tp3 + fp3), 5)\n",
    "recall3 = round(tp3 / (tp3 + fn3), 5)\n",
    "\n",
    "\n",
    "preda3 = model3.predict_proba(x_test3)[:, 1]\n",
    "roc_auc3 = round(roc_auc_score(y_test3, preda3), 5)\n",
    "\n",
    "preda2 = model2.predict_proba(x_test2)[:, 1]\n",
    "roc_auc2 = round(roc_auc_score(y_test2, preda2), 5)\n",
    "\n",
    "\n",
    "def compare_results():\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            [f\"{precision}\", f\"{recall}\", f\"{roc_auc}\"],\n",
    "            [f\"{precision2}\", f\"{recall2}\", f\"{roc_auc2}\"],\n",
    "            [f\"{precision3}\", f\"{recall3}\", f\"{roc_auc3}\"],\n",
    "        ],\n",
    "        columns=[\"Precision\", \"Recall\", \"roc_auc\"],\n",
    "        index=[\"No Mitigation\", \"With Mitigation\", \"With Mitigation & Rebalance\"],\n",
    "    )\n",
    "\n",
    "\n",
    "compare_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
