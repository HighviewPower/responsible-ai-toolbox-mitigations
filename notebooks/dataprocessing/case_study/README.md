# Exploring real datasets using the dataprocessing library

This folder contains a few examples of possible ways to use the **dataprocessing** library to explore real-world datasets. Each dataset is considered here as a real-world case study and are numbered. Each case study X has two notebooks associated to it:

* **caseX.ipynb:** a standard exploration and manipulation of the dataset using some of the features available in the dataprocessing library, such as: categorical data encoding, missing data imputation, feature selection, and synthetic data generation for imbalanced classes. These notebooks will start off by doing a simple data exploration, printing the data types in the dataset and exploring some specific features. It then encodes categorical data using either ordinal encoding or one-hot encoding (depends on the dataset) and imputes missing data, if necessary. The next step is to build a baseline model by splitting the dataset into train and test sets, fitting a model over the training data, and then retrieving some metrics from the test set. After reporting the results obtained for the baseline model, the notebook starts making greater modifications to the dataset by using feature selection and generating artificial data, where a new model is generated for each step. This allows us to learn how to use each of these data processing functionalities that are made available by the dataprocessing library, and also understand the impact that these features offers to the final model;
* **caseX_stat.ipynb:** in the notebook *caseX.ipynb*, we only run each model once, while testing only a single data split between train and test datasets. However, depending on the dataset or on the model used, the data split or certain randomness inherent to the model training process may results in very different results. These fluctuations on the results obtained for a given data split and trained model may be misleading when trying to understand the impact of certain data modification functionalities, such as the ones offered by the dataprocessing library. Therefore, in this notebook, we create a pipeline specific to each dataset that allows us to use different data splits and train different models, and then collect the average result obtained from all the different models trained. We then plot the AUC ROC, precision, recall, and F1 score for these pipelines, and then compare different data pipelines. Each data pipeline consists of several data splits and trained models after performing a set of modifications in the dataset. For example, one pipeline only encodes categorical data and imputes missing values, while another pipeline extends these two operations by performing feature selection. We created a different pipeline using each of the functionalities available in the dataprocessing library, allowing us to see how these functionalities impact the model results on average;