{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Data Balance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "Data Balance Analysis is relevant for overall understanding of datasets, but it becomes essential when thinking about building Machine Learning models in a responsible way, especially in term of fairness. It is all too easy to build an ML Model that produced biased results for subsets of the population by training or testing the model of biased ground truth data. There are multiple case studies of biased models assisting in granting loans healthcare, recruitment opportunities and many other decision making tasks. In most of these examples, the data from which these models are trained was the common issue. These findings emphasize how important it is for model creators and auditors to analysis data balance: to measure training data across various sub-populations and ensure the data has good coverage and a balanced representation of labels across sensitive categories adn category combinations and to check that test data is representative of the target population\n",
    "\n",
    "In summary, Data Balance Analysis when used a step for building ML models has the following benefits: \n",
    "- reduces the risk of unbalanced models, ensuring service fairness and reducing the costs of ML building by identifying data representation gaps early on and prompting data scientists to seek mitigation steps before proceeding on the training portion of Machine Learning model development\n",
    "- Enables easy end-to-end debugging of ML systems in combination with Fairlearn by providing a clear view if an issue in a model is tied to the data or the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage \n",
    "Data Balance Analysis supports three different types of metrics. \n",
    "- FeatureMeasures - supervised (requires a label column)\n",
    "- DistributionMeasures - unsupervised (does not require a label column)\n",
    "- AggregateMeasures - unsupervised (does not require a label column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Notebook\n",
    "[Adult Census Income Dataset] () "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we import all of the classes we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Akshara/Desktop/RAI/responsible-ai-mitigations\n",
      "Requirement already satisfied: pandas in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from raimitigations==0.0.0) (1.2.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from raimitigations==0.0.0) (1.21.4)\n",
      "Requirement already satisfied: wheel in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from raimitigations==0.0.0) (0.37.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from raimitigations==0.0.0) (1.6.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from raimitigations==0.0.0) (1.0.1)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from raimitigations==0.0.0) (0.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from imbalanced-learn->raimitigations==0.0.0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from scikit-learn->raimitigations==0.0.0) (3.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from pandas->raimitigations==0.0.0) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from pandas->raimitigations==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akshara\\anaconda3\\envs\\ischiadev\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->raimitigations==0.0.0) (1.16.0)\n",
      "Installing collected packages: raimitigations\n",
      "  Attempting uninstall: raimitigations\n",
      "    Found existing installation: raimitigations 0.0.0\n",
      "    Uninstalling raimitigations-0.0.0:\n",
      "      Successfully uninstalled raimitigations-0.0.0\n",
      "  Running setup.py develop for raimitigations\n",
      "Successfully installed raimitigations-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../../../responsible-ai-mitigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raimitigations.databalanceanalysis import FeatureBalanceMeasure, DistributionBalanceMeasure, AggregateBalanceMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the dataset, define the features of interest and ensure that the label column is binary. Currently, the FeatureBalance measure calculator only supports binary labels. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sensitive_features = [\"Gender\", \"Race\"]\n",
    "\n",
    "df =  pd.read_csv('../../datasets/AdultCensusIncome.csv')\n",
    "\n",
    "# convert to 0 and 1 encoding\n",
    "df['income'] = df['income'].apply(lambda x: 0 if x == \"<=50K\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create an instance of the FeatureMeasure class and set the sensitives to the column you are interested in seeing and the label column to the name of the column of interest.\n",
    "\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_of_interest = ['race', 'sex']\n",
    "label_col = \"income\"\n",
    "feature_measures = FeatureBalanceMeasure(cols_of_interest, label_col)\n",
    "feat_measures = feature_measures.measures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create an instance of the DistributionMeasures class and set the sensitive columns to the columns you are interested in seeing. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_measures = DistributionBalanceMeasure(['race', 'sex'])\n",
    "distribution_measures.measures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create an instance of the AggregateMeasures class and set the sensitive columns parameter to the columns of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['income'] = df['income'].apply(lambda x: 0 if x == \"<=50K\" else 1)\n",
    "aggregate_measures = AggregateBalanceMeasure(['race'])\n",
    "aggregate_measures.measures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations of Data Balance Measures\n",
    "### Feature Balance Measures\n",
    "Feature Balance Measures allow us to see whether each combination of sensitive feature is receiving the positive outcome (true prediction) at balanced probability.\n",
    "\n",
    "In this context, we define a feature balance measure, also referred to as the parity, for label y as the difference between the association metrics of two different sensitive classes $([x_A, x_B])$, with respect to the association metric $(A(x_i, y))$. That is:\n",
    "\n",
    "$$parity(y \\vert x_A, x_B, A(\\cdot)) \\coloneqq A(x_A, y) - A(x_B, y) $$\n",
    "\n",
    "Using the dataset, we can see if the various sexes and races are receiving >50k income at equal or unequal rates.\n",
    "\n",
    "Note: Many of these metrics were influenced by this paper [Measuring Model Biases in the Absence of Ground Truth.](https://arxiv.org/abs/2103.03417)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Association Metric | Family | Description | Interpretation | Reference\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Demographic Parity | Fairness | Proportion of each segment of a protected class (e.g. gender) should receive the positive outcome at equal rates.\t| As close to 0 means better parity. $(DP = P(Y \\vert A = Male) - P(Y \\vert A = Female))$. Y = Positive label rate.| [Link](https://en.wikipedia.org/wiki/Fairness_%28machine_learning%29) |\n",
    "|Pointwise Mutual Information (PMI), normalized PMI | Entropy\t|The PMI of a pair of feature values (ex: Gender=Male and Gender=Female) quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions (assuming independence). | Range (normalized) [-1, 1]. -1 for no co-occurences. 0 for co-occurences at random. 1 for complete co-occurences.| [Link](https://en.wikipedia.org/wiki/Pointwise_mutual_information) |\n",
    "| Sorensen-Dice Coefficient (SDC) | Intersection-over-Union| Union\tUsed to gauge the similarity of two samples. Related to F1 score. |Equals twice the number of elements common to both sets divided by the sum of the number of elements in each set. | [Link](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) | \n",
    "|Jaccard Index | Intersection-over-Union | Similar to SDC, guages the similarity and diversity of sample sets. | Equals the size of the intersection divided by the size of the union of the sample sets. | [Link](https://en.wikipedia.org/wiki/Jaccard_index) | \n",
    "|Kendall Rank Correlation | Correlation and Statistical Tests | Used to measure the ordinal association between two measured quantities. | High when observations have a similar rank and low when observations have a dissimilar rank between the two variables. |[Link](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) |\n",
    "|Log-Likelihood Ratio | Correlation and Statistical Tests |  Statistical Tests\tCalculates the degree to which data supports one variable versus another. Log of the likelihood ratio, which gives the probability of correctly predicting the label in ratio to probability of incorrectly predicting label. | If likelihoods are similar, it should be close to 0. |[Link](https://en.wikipedia.org/wiki/Likelihood_function#Likelihood_ratio) | \n",
    "|t-test | Correlation and Statistical Tests | Used to compare the means of two groups (pairwise). | Value looked up in t-Distribution tell if statistically significant or not. |[Link](https://en.wikipedia.org/wiki/Student's_t-test) | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Balance Measures\n",
    "\n",
    "Distribution Balance Measures allow us to compare our data with a reference distribution (currently only uniform distribution is supported as a reference distribution). They are calculated per sensitive column and do not depend on the label column.\n",
    "\n",
    "For example, let's assume we have a dataset with 9 rows and a Gender column, and we observe that:\n",
    "\n",
    "* \"Male\" appears 4 times\n",
    "* \"Female\" appears 3 times\n",
    "* \"Other\" appears 2 times\n",
    "\n",
    "Assuming the uniform distribution:\n",
    "$$ReferenceCount \\coloneqq  \\frac{numRows}{numFeatureValues}$$\n",
    "$$ReferenceProbability \\coloneqq  \\frac{1}{numFeatureValues}$$\n",
    "\n",
    "Feature Value | Observed Count | Reference Count | Observed Probability | Reference Probabiliy\n",
    "| - | - | - | - | -\n",
    "Male | 4 | 9/3 = 3 | 4/9 = 0.44 | 3/9 = 0.33\n",
    "Female | 3 | 9/3 = 3 | 3/9 = 0.33 | 3/9 = 0.33\n",
    "Other | 2 | 9/3 = 3 | 2/9 = 0.22 | 3/9 = 0.33\n",
    "\n",
    "We can use distance measures to find out how far our observed and reference distributions of these feature values are. Some of these distance measures include:\n",
    "\n",
    "Measure | Description | Interpretation | Reference\n",
    "| - | - | - | -\n",
    "KL Divergence | Measure of how one probability distribution is different from a second, reference probability distribution. Measure of the information gained when one revises one's beliefs from the prior probability distribution Q to the posterior probability distribution P. In other words, it is the amount of information lost when Q is used to approximate P. | Non-negative. 0 means P = Q. | [Link](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "Jenson-Shannon Distance | Measuring the similarity between two probability distributions. Symmetrized and smoothed version of the Kullback–Leibler (KL) divergence. Square root of Jenson-Shannon Divergence. | Range [0, 1]. 0 means perfectly same to balanced distribution. | [Link](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)\n",
    "Wasserstein Distance | This distance is also known as the earth mover’s distance, since it can be seen as the minimum amount of “work” required to transform u into v, where “work” is measured as the amount of distribution weight that must be moved, multiplied by the distance it has to be moved. | Non-negative. 0 means P = Q. | [Link](https://en.wikipedia.org/wiki/Wasserstein_metric)\n",
    "Infinity Norm Distance | Distance between two vectors is the greatest of their differences along any coordinate dimension. Also called Chebyshev distance or chessboard distance. | Non-negative. 0 means same distribution. | [Link](https://en.wikipedia.org/wiki/Chebyshev_distance)\n",
    "Total Variation Distance | It is equal to half the L1 (Manhattan) distance between the two distributions. Take the difference between the two proportions in each category, add up the absolute values of all the differences, and then divide the sum by 2. | Non-negative. 0 means same distribution. | [Link](https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures)\n",
    "Chi-Squared Test | The chi-square test tests the null hypothesis that the categorical data has the given frequencies given expected frequencies in each category. | p-value gives evidence against null-hypothesis that difference in observed and expected frequencies is by random chance. | [Link](https://en.wikipedia.org/wiki/Chi-squared_test)|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Balance Measures\n",
    "\n",
    "Aggregate Balance Measures allow us to obtain a higher notion of inequality. They are calculated on the set of all sensitive columns and do not depend on the label column.\n",
    "\n",
    "These measures look at distribution of records across all combinations of sensitive columns. For example, if Sex and Race are specified as sensitive features, it then tries to quantify imbalance across all combinations of the two specified features - (Male, Black), (Female, White), (Male, Asian-Pac-Islander), etc.\n",
    "\n",
    "Measure | Description | Interpretation | Reference\n",
    "| - | - | - | -\n",
    "Atkinson Index | It presents the percentage of total income that a given society would have to forego in order to have more equal shares of income between its citizens. This measure depends on the degree of society aversion to inequality (a theoretical parameter decided by the researcher), where a higher value entails greater social utility or willingness by individuals to accept smaller incomes in exchange for a more equal distribution. An important feature of the Atkinson index is that it can be decomposed into within-group and between-group inequality. | Range [0, 1]. 0 if perfect equality. 1 means maximum inequality. In our case, it is the proportion of records for a sensitive columns’ combination. | [Link](https://en.wikipedia.org/wiki/Atkinson_index)\n",
    "Theil T Index | GE(1) = Theil's T and is more sensitive to differences at the top of the distribution. The Theil index is a statistic used to measure economic inequality. The Theil index measures an entropic \"distance\" the population is away from the \"ideal\" egalitarian state of everyone having the same income. | If everyone has the same income, then T_T equals 0. If one person has all the income, then T_T gives the result (ln N). 0 means equal income and larger values mean higher level of disproportion. | [Link](https://en.wikipedia.org/wiki/Theil_index)\n",
    "Theil L Index | GE(0) = Theil's L and is more sensitive to differences at the lower end of the distribution. Logarithm of (mean income)/(income i), over all the incomes included in the summation. It is also referred to as the mean log deviation measure. Because a transfer from a larger income to a smaller one will change the smaller income's ratio more than it changes the larger income's ratio, the transfer-principle is satisfied by this index. | Same interpretation as Theil T Index. | [Link](https://en.wikipedia.org/wiki/Theil_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation\n",
    "\n",
    "It will not be a stretch to say that every real-world dataset has caveats, biases, and imbalances. Data collection is costly. Data Imbalance mitigation or de-biasing data is an area of research. There are many techniques available at various stages of ML lifecycle i.e., during pre-processing, in-processing, and post processing. Here we outline a couple of pre-processing techniques -  \n",
    "\n",
    "### Resampling\n",
    "\n",
    "This involves under-sampling from majority class and over-sampling from minority class. Most naïve way to over-sample would be duplicate records and under-sample would be to remove records at random.  \n",
    "\n",
    "* Caveats:  \n",
    "\n",
    "  1. Under-sampling may remove valuable information.  \n",
    "  2. Over-sampling may cause overfitting and poor generalization on test set.\n",
    "\n",
    "![Bar chart undersampling and oversampling](https://mmlspark.blob.core.windows.net/graphics/responsible_ai/DataBalanceAnalysis_SamplingBar.png)\n",
    "\n",
    "There are smarter techniques to under-sample and over-sample in literature and implemented in Python’s [imbalanced-learn](https://imbalanced-learn.org/stable/) package.  \n",
    "\n",
    "For example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information.  \n",
    "\n",
    "One technique of under-sampling is use of Tomek Links. Tomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process. A similar way to under-sample majority class is using Near-Miss. It first calculates the distance between all the points in the larger class with the points in the smaller class. When two points belonging to different classes are very close to each other in the distribution, this algorithm eliminates the datapoint of the larger class thereby trying to balance the distribution.\n",
    "\n",
    "![Tomek Links](https://mmlspark.blob.core.windows.net/graphics/responsible_ai/DataBalanceAnalysis_TomekLinks.png)\n",
    "\n",
    "In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples. This technique is called SMOTE (Synthetic Minority Oversampling Technique). It randomly picks a point from the minority class and computes the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "\n",
    "![Synthetic Samples](https://mmlspark.blob.core.windows.net/graphics/responsible_ai/DataBalanceAnalysis_SyntheticSamples.png)\n",
    "\n",
    "### Reweighting\n",
    "\n",
    "There is an expected and observed value in each table cell. The weight is essentially expected / observed value. This is easy to extend to multiple features with more than 2 groups. The weights are then incorporated in loss function of model training.  \n",
    "\n",
    "![Reweighting](https://mmlspark.blob.core.windows.net/graphics/responsible_ai/DataBalanceAnalysis_Reweight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: This notebook is adaptation of this notebook in the SynapseML documentation https://microsoft.github.io/SynapseML/docs/features/responsible_ai/Data%20Balance%20Analysis/ written by Kashyap Patel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25553ff2aeabf811aa6253044d4e27aa04c210fdd185f9aa4a44c307365de193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ischiadev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
