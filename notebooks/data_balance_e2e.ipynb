{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Data Balance and Error Mitigation\n",
    "This Notebook will demonstrate how to use both the Data Balance Analysis capabilities and error mitigation functions together using an example HR dataset which is a tabular dataset with a label column that indicates whether or not a person is promoted based on attributes such as education, gender, number of trainings, and other factors. \n",
    "The steps that we will take in this notebook are \n",
    "1. We will first conduct an analysis on how balanced the data is. \n",
    "2. We will train an example model to see how it performs on the data. \n",
    "3. We will try to balance the data to mitigate biases that may have resulted from unbalanced data\n",
    "4. We will then compare model performance and data balance metrics before and after rebalancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the dependencies needed in our analysis. This includes the classes to produce the data balance metrics, the sklearn functions to see the model performance and the error mitigation steps like DataRebalance and DataSplit that we apply to the dataset itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -e ../../responsible-ai-mitigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from raimitigations.dataprocessing import Rebalance, Split\n",
    "\n",
    "from raimitigations.databalanceanalysis import FeatureBalanceMeasure, AggregateBalanceMeasure, DistributionBalanceMeasure\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM Model\n",
    "Now we import the tabular dataset that we will look at in the example, we load it into a pandas dataframe that we can then modify and use for all the other steps. For the data balance analysis portion we need our label columns and a list of sensitive columns that are interested in checking for balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "outdirname = 'mitigations-datasets.2.22.2022'\n",
    "zipfilename = outdirname + '.zip'\n",
    "if not pathlib.Path(outdirname).exists () :\n",
    "    urlretrieve('https://publictestdatasets.blob.core.windows.net/data/' + zipfilename, '../../' + zipfilename)\n",
    "    with zipfile.ZipFile('../../' + zipfilename, 'r') as unzip:\n",
    "        unzip.extractall('../../.')\n",
    "\n",
    "data_dir = ('../' + outdirname + '/hr_promotion')\n",
    "df =  pd.read_csv(data_dir + '/train.csv').drop(['employee_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some data transformation on the categorical columns in order to make the training data input in the format that the lightGBM model expects. Although lightGBM can internally deal with categorical columns that the user specifies, we need to encode those categories into integers before we are able to train the lightGBM model on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = ['education', 'recruitment_channel']\n",
    "categorical_cols = [\"department\", \"gender\", \"education\", \"region\", \"recruitment_channel\" ]\n",
    "label_col = 'is_promoted'\n",
    "seed = 42\n",
    "# handle duplicates\n",
    "df = df.drop_duplicates().drop([\"employee_id\"], axis = 1)\n",
    "df = df.dropna()\n",
    "ord_enc = OrdinalEncoder(dtype = int)\n",
    "df[categorical_cols] = ord_enc.fit_transform(df[categorical_cols])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a split on the data, train a LightGBM model, and see how this model does on some test data. After this processing, we train the model and we can see that the model does well on false values, getting 97.3% of them correct, but the model does a lot worse on the true values, only identifying approximately a third of the true positives correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a model and get accuracy numbers\n",
    "\n",
    "# data prep\n",
    "def split_label(dataset):\n",
    "    x = dataset.drop(['is_promoted'], axis=1)\n",
    "    y = dataset['is_promoted']\n",
    "    return x, y\n",
    "\n",
    "dataset = df\n",
    "target_index = dataset.columns.get_loc('is_promoted')\n",
    "data_split =  Split(dataset,target_index , 0.9, 42, False, False, False, True)\n",
    "train_data, test_data = data_split.split()\n",
    "# splitting the training data\n",
    "x_train, y_train = split_label(train_data)\n",
    "# splitting the test data\n",
    "x_test, y_test = split_label(test_data)\n",
    "\n",
    "# LGBMClassifier Model\n",
    "clf = LGBMClassifier(n_estimators=50, )\n",
    "model = clf.fit(x_train, y_train, categorical_feature = categorical_cols)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, normalize='true')\n",
    "    return pd.DataFrame([[f'TP = {tp} ({tpr:1.2%})', f'FN = {fn} ({fnr:1.2%})'], \n",
    "                         [f'FP = {fp} ({fpr:1.2%})', f'TN = {tn} ({tnr:1.2%})']],\n",
    "                        index=['True', 'False'], \n",
    "                        columns=['Pred 1', 'Pred 0'])\n",
    "\n",
    "print(\"number of errors on test dataset: \" + str(sum(pred != y_test)))\n",
    "\n",
    "print(conf_matrix(y_test,pred))\n",
    "\n",
    "print(classification_report(y_test, pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis on Baseline Model\n",
    "Now that we have a baseline model to work with, we can see how this model is doing overall on the data and see if there are any cohorts within the data that it performs worse on. We use the [Error Analysis Dashboard](https://erroranalysis.ai/) to determine which cohorts of data this model performs worse on. Since the error analysis dashboard is interactive and too large to render on Github, we will include screenshots from our analysis. From these screenshots we can see that if we zoom in on certain cohorts that the model is getting more errors on, that region, department and education are all attributes that are involved in those cohorts. For the purpose of this example, we chose remove some of the other columns like KPIs_met from the error analysis since we want to focus on attributes that may lead to biases rather than more measurable attributes. We will focus on analyzing and mitigating errors within the department and education columns for the rest of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ErrorAnalysisDashboard\n",
    "predictions = model.predict(x_test)\n",
    "#ErrorAnalysisDashboard(dataset=x_test, true_y=y_test, features=x_test.columns, pred_y=predictions, categorical_features = categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error_analysis1](images/error_analysis1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balance Analysis\n",
    "First we can take a look at the feature balance measures. These measures indicate the difference in the label column amongst different feature values. For example the first row here indicates if people with the \"Masters & above\" education has a different proportion of people receiving the promoted outcome than those that have a Bachelor's. Lower values of these measures indicates that the amounts of people with class A vs versus those with class B with a label of 1 is similar. The t-test value can also tell us if the difference we see is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_data\n",
    "train_df[categorical_cols ] = ord_enc.inverse_transform(train_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_measures = FeatureBalanceMeasure( cols_of_interest, label_col)\n",
    "\n",
    "feat_measures1 = feature_measures.measures(train_df)\n",
    "feat_measures1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " %matplotlib inline \n",
    "educations = train_df['education'].unique()\n",
    "education_dp_values = feat_measures1[feat_measures1[\"FeatureName\"] == 'education'][[\"ClassA\", \"ClassB\", \"pmi\"]]\n",
    "education_dp_array = np.zeros((len(educations), len(educations)))\n",
    "\n",
    "for idx, row in education_dp_values.iterrows():\n",
    "    class_a = row[0]\n",
    "    class_b = row[1]\n",
    "    dp_value = row[2]\n",
    "    i, j = np.where(educations==class_a)[0][0], np.where(educations == class_b)\n",
    "    dp_value = round(dp_value, 2)\n",
    "    education_dp_array[i, j] = dp_value\n",
    "    education_dp_array[j, i] = -1 * dp_value\n",
    "\n",
    "colormap = \"RdBu\"\n",
    "dp_min, dp_max = -1.0, 1.0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(education_dp_array, vmin=dp_min, vmax=dp_max, cmap=colormap)\n",
    "\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Point Mutual Info\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "ax.set_xticks(np.arange(len(educations)))\n",
    "ax.set_yticks(np.arange(len(educations)))\n",
    "ax.set_xticklabels(educations)\n",
    "ax.set_yticklabels(educations)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(educations)):\n",
    "    for j in range(len(educations)):\n",
    "        text = ax.text(j, i, education_dp_array[i, j], ha=\"center\", va=\"center\", color=\"k\")\n",
    "    \n",
    "ax.set_title(\"PMI of education in HR Dataset\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can take a look at the distribution balance measures. These measures each of the columns of interest that we selected to the uniform distribution of those values. Values that are closer to zero indicate that the difference between the actual distribution of the data and the uniform distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_measures = DistributionBalanceMeasure( cols_of_interest)\n",
    "dist_measures1 = dist_measures.measures(train_df)\n",
    "dist_measures1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline \n",
    "measures_of_interest = [\"kl_divergence\", \"js_dist\", \"inf_norm_dist\", \"total_variation_dist\", \"wasserstein_dist\"]\n",
    "education_measures = dist_measures1[dist_measures1['FeatureName'] == 'education'].iloc[0]\n",
    "department_measures = dist_measures1[dist_measures1['FeatureName'] == 'recruitment_channel'].iloc[0]\n",
    "education_array = [round(education_measures[measure], 4) for measure in measures_of_interest]\n",
    "department_array = [round(department_measures[measure], 4) for measure in measures_of_interest]\n",
    "\n",
    "x = np.arange(len(measures_of_interest))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, education_array, width, label=\"education\")\n",
    "rects2 = ax.bar(x + width/2, department_array, width, label=\"recruitment channel\")\n",
    "\n",
    "ax.set_xlabel(\"Measure\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Distribution Balance Measures of Education and Recruitment Channel in Adult Dataset\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(measures_of_interest)\n",
    "ax.legend()\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=20, ha=\"right\", rotation_mode=\"default\")\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 1),  # 1 point vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at aggregate balance measures which indicate a notion of overall inequality in the data. We can see that the Atkinson Index is 0.79. This means that in order to create a perfectly balanced dataset over these measures we would need to forgo 79.9% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_measures = AggregateBalanceMeasure( cols_of_interest)\n",
    "agg_measures1 = agg_measures.measures(train_df)\n",
    "agg_measures1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Mitigation: Rebalancing dataset\n",
    "In order to rebalance the data we can choose from three different methods of under or oversampling. These are SMOTE, Tomek and SMOTE-Tomek. SMOTE is a oversampling technique for the less represented class. Tomek is an undersampling technique that would be applied to the more represented class. Smote-Tomek is when both of these methods are applied in conjunction on the dataset. In this example, we will choose to use the SMOTE sampling technique on the columns of interest. The Rebalance function can only be applied on one column at a time so in order to apply this rebalancing technique on a cohort of two sensitive columns instead of an individual column, we combine these two columns into a single column that can be balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "# these are the other balance algorithm objects we could use but\n",
    "#  we are using the SMOTE resampling technique in this example\n",
    "# smote_tomek = SMOTETomek()\n",
    "# tomek = TomekLinks()\n",
    "\n",
    "def combine_cols(df):\n",
    "    return str(df[0]) + \" * \" + str(df[1])\n",
    "\n",
    "train_df2 = train_df\n",
    "train_df2[categorical_cols] = ord_enc.transform(train_df[categorical_cols])\n",
    "train_df2[\"education_recruitment_cohort\"] = df[[\"education\", \"recruitment_channel\"]].apply(combine_cols, axis=1)\n",
    "train_df2 = train_df2.drop([\"education\", \"recruitment_channel\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balance_smote = Rebalance(train_df2, 'education_recruitment_cohort', 'auto', 42, None, smote, None)\n",
    "\n",
    "smote_df = data_balance_smote.rebalance()\n",
    "smote_df['education'] = smote_df['education_recruitment_cohort'].apply(lambda x: int(x.split(\" * \")[0]))\n",
    "smote_df['recruitment_channel'] = smote_df['education_recruitment_cohort'].apply(lambda x: int(x.split(\" * \")[1]))\n",
    "smote_df = smote_df.drop([\"education_recruitment_cohort\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO size of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Model on Rebalanced Datasets\n",
    "After applying the SMOTE Method on the data, we can then train a new lightGBM model on this newly balanced data and see if there are differences in model performance based on this balancing. We compare the results below and find that the new model trained on the data post rebalancing does a better job predicting true positives than the original model and thus has greater recall and overall precision. So not only does data rebalancing help with making sure a model is less biased, it also helps the model actually fit and be able to predict the data outcomes more accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = smote_df.columns.get_loc('is_promoted')\n",
    "data_split =  Split(smote_df,target_index , 0.9, 42, False, False, False, True)\n",
    "train_data, test_data = data_split.split()\n",
    "# splitting the training data\n",
    "x_train2, y_train2 = split_label(train_data)\n",
    "# splitting the test data\n",
    "x_test2, y_test2 = split_label(test_data)\n",
    "\n",
    "# LGBMClassifier Model\n",
    "clf2 = LGBMClassifier(n_estimators=50)\n",
    "model2 = clf2.fit(x_train2, y_train2, categorical_feature = categorical_cols)\n",
    "\n",
    "pred2 = model2.predict(x_test2)\n",
    "# reorders the columns to fit the second model since there is some rearranging\n",
    "pred_model1 = model.predict(x_test2.reindex(columns = x_test.columns))\n",
    "\n",
    "def conf_matrix(y,pred):\n",
    "    ((tn, fp), (fn, tp)) = metrics.confusion_matrix(y, pred)\n",
    "    ((tnr,fpr),(fnr,tpr))= metrics.confusion_matrix(y, pred, normalize='true')\n",
    "    return pd.DataFrame([[f'TP = {tp} ({tpr:1.2%})', f'FN = {fn} ({fnr:1.2%})'], \n",
    "                         [f'FP = {fp} ({fpr:1.2%})', f'TN = {tn} ({tnr:1.2%})']],\n",
    "                        index=['True', 'False'], \n",
    "                        columns=['Pred 1', 'Pred 0'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the number of error that the model 1 that was trained before rebalancing and model 2 that was trained after rebalancing have and we find that overall there are less errors with model 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Results\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "\n",
    "print('')\n",
    "print(color.PURPLE + color.BOLD + \"BEFORE: \" + color.END + \"number of test dataset instances: \" + color.BOLD   + color.GREEN + str(len(y_test2)) + color.END)\n",
    "print(\"      : number of errors on test dataset: \" + color.BOLD   + color.RED + str(sum(pred_model1 != y_test2)) + color.END)\n",
    "print('')\n",
    "print(color.PURPLE + color.BOLD + \"AFTER:  \" + color.END + \"number of test dataset instances: \" + color.BOLD   + color.GREEN + str(len(y_test2)) + color.END)\n",
    "print(\"     :  number of errors on test dataset: \" + color.BOLD  + color.RED + str(sum(pred2 != y_test2)) + color.END)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare conf matrices\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print('')\n",
    "print(color.BLUE + color.BOLD +\"BEFORE: conf_matrix:\" + color.END)\n",
    "print(\"--------------------\")\n",
    "print(conf_matrix(y_test2,pred_model1) )\n",
    "print('')\n",
    "print(color.BLUE + color.BOLD +\"AFTER: conf_matrix:\" + color.END)\n",
    "print(\"-------------------\")\n",
    "print(conf_matrix(y_test2,pred2))\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO call out which improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare classification report\n",
    "print(color.YELLOW + color.BOLD +\"BEFORE: classification_report:\" + color.END)\n",
    "print(\"--------------------------------\")\n",
    "print(classification_report(y_test2, pred_model1)) \n",
    "print(color.YELLOW + color.BOLD +\"AFTER: classification_report:\" + color.END)\n",
    "print(\"--------------------------------\")\n",
    "print(classification_report(y_test2, pred2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return the dataframe columns to their original form after rebalancing the data. We no longer encode  We want to be able to run the data balance analysis again on the newly rebalanced data and see what the difference is from before applying SMOTE and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_df[categorical_cols] = ord_enc.inverse_transform(smote_df[categorical_cols]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature value measures before rebalancing don't indicate a lot of discrepancy in the outcome within specific features within a class since the values such as demographic parity which is on a 0 to 1 scale are very close to zero. After rebalancing, we still have similarly low values of these measures but there is not a significant improvement since they already started low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo feat balance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_measures1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_measures.measures(smote_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the distribution measures before and after rebalancing, we find that the data is much more evenly distributed (close to the uniform distribution) for the two columns of interest after rebalancing the data using the SMOTE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#before\n",
    "dist_measures1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#after\n",
    "dist_measures.measures(smote_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Atkinson index which gives us the overall notion of inequality before and after rebalancing shows us that in order to get a perfectly balanced dataset, we no longer need to forgo any of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before\n",
    "agg_measures1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after\n",
    "agg_measures.measures(smote_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = clf.fit(x_train, y_train)\n",
    "predictions2 = model2.predict(x_test)\n",
    "#ErrorAnalysisDashboard(dataset=x_test, true_y=y_test, features=x_test.columns, pred_y=predictions2, categorical_features = categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label the top as before and the bottom as after\n",
    "highlight underneath the cohort\n",
    "lower fill (meaning )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![error_analysis2](images/error_analysis2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25553ff2aeabf811aa6253044d4e27aa04c210fdd185f9aa4a44c307365de193"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
